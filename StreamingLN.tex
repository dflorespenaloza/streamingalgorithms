\documentclass[12pt]{article}


\usepackage{parskip} \setlength{\parindent}{15pt}


\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
%\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{claim}{Claim}
\newtheorem{remark}{Remark}
\newtheorem{question}{Question}
\newtheorem{definition}{Definition}


\title{Streaming Algorithms}
\author{The Authors}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle


%%---------------------------------------------------------------------
\section{Rewarding Subway's users}

\subsection{The problem}

\subsection{What is Big Data?}

\subsection{The streaming model}

\subsection{A simple algorithm}

\subsection{Exercises}


%%---------------------------------------------------------------------
\section{Heavy hitters {\small [Manku and Motwani 2002]}}


\subsection{The problem}

\subsection{A warm up: find the majority element}


\subsection{A heavy hitters algorithm}


\subsection{Exercises}


%%---------------------------------------------------------------------

\section{Length of the stream with few space {\small [Morris 1978, Flajolet 1985]}}

\subsection{Definition of the problem}

\subsection{Applications}

\subsection{A simple implementation (not in the streaming model)}

\subsection{Morris's algorithm} $\,$

\subsection{Some ideas to take away from Flajolet's analysis}

\subsection{Exercises}


%%---------------------------------------------------------------------
\section{Number of distinct elements}

\subsection{The problem}

\begin{definition}[The number of distinct elements problem]
The input is an stream $S$ with $m$ elements, each taken from $\{1, \hdots, n\}$.
After reading the stream once, we want to know the number of distinct elements in $S$.
\end{definition}

This problem is a particular case of the so-called \emph{frequency moments of} $S$:


\begin{definition}[Frequency moments of a stream]
Let $S = s_1, \hdots, s_m$ be a string with $m$ elements, each taken from $\{1, \hdots, n\}$.
Let $m_i$ be the number of apparences of $i$ in $S$, namely, $m_i = | \{j : s_j = i\} |$.
For every integer $k \geq 0$, the \emph{$k$-th frequency moment of $S$} is

$$F_k = \Sigma^n_{i = n} m_i^k.$$ 
\end{definition}


Thus $F_0$ is the number of distinct elements of $S$ while $F_1$ is its length.
The second frequency momento $F_2$ is called the \emph{surprise index}
and gives interesting information of a stream that is useful in several statistical applications.
The next lecture presents the surprise index more in detail.

\subsection{An algorithm}

The following simple and nice algorithm from Alon et al. estimates the number of distinct elements of a stream.
It does so by estimating the number of bits needed to encode the elements in the string.

The algorithm first picks a finite field large enough where all distinct values of the stream can be
represented. Then, it picks at random a hash function (a polynomial) that maps each elemento to
a member in the field. 
For each element in the stream, the algorithm applies the hash function and then
looks at the number consecutive 0's at the right of the binary representation.
That number is what the algorithm is interested in, it returns the largest number it sees.

Here it is important to remark that once a hash function is chosen, the algorithm is deterministic. In the analysis of 
the algorithm (following section) it is important that for every element of the string $s_i$,
$z_i$ (the value $s_i$ is mapped to) is uniformly distributed over ${0, ..., 2^d-1}$.
Something that also is important about the hash function is that it is pairwise independent,
a notion that is explained in the next section.




\begin{verbatim}
d = smallest integer so that 2^d > n
a,b = two random values in {0, ..., 2^d-1} chosen uniformly and independently
r = 0

while(there are elements in the stream)
   s_i = next in the stream
   z_i = a s_i + b (product and addition are computed in the field GF(2^d))
   r(z_i) = largest j so that the j rightmost bits of z_i are all 0
   r = max{r(z_i), r}       
endwhile

return r

\end{verbatim}



\subsection{What is the algorithm computing?}




\subsection{Not a formal analysis but so ideas to take away}

Intuitively, the algorithm works because:
\begin{itemize}
\item On one hand, the probability that \emph{one} hashed number has $r$ zero right-most  bits, decreases exponentiallr fast on $r$.
\item on the other hand, the probability that \emph{some} hashed number has $r$ zero right-most bits, increases exponentially fast on $r$.
\end{itemize}
These two probabilities are \emph{balanced} in such a way that that the probability of giving the right answer is "high".

The exact details are of algebraic nature and can be presented within 30 minutes, taking a slow pace to make sure the students can understand. Of course we are assuming they have almost no previous exposure to probability, just as yours truly.

Markov's and Chebyshev's inequalitys are used to bound the probability that the algorithm fails.

\subsubsection{pairwise independence}

Something not used as often as Markov's and Chebyshev's inequalitys, which is used and is very important 
is the concept of \emph{pairwise independence}. During the proof, the variance of one random variable is calculated. 
This variance depends on the fact that the probabilities that two distinct input numbers $a_i$ and $a_j$ are both mapped to numbers with at least $r$ zero right-most bits, are independent.

In this particular algorithm, this is guaranteed because of the nature of our hashing functions.

To understand what a pair-wise (or $k$-wise independent)  random distribution is, is easier to first see an example of a distribution that is not. 
Consider a  random variable $X$ of two random bits, each bit having uniform probability for having one of zero.

We can define $X$ as having a value of the set $\{00,11\}$ with uniform probability. This random variable satisfies the required property. Nevertheles, the two bits of $X$ are always equal. The probability of $X$ is not uniformly distributed on the set of two bit values $\{00,01,10,11\}$.
Thus, $X$ has no more \emph{randomness} than a single bit random variable.

Now consider $X$ defined as having a value of the set $\{00,01,10,11\}$ with uniform probability. Now, the selection of the first bit can be seen as completely independent of the selection of the second bit: given the value of the first bit, the probabilities  of the second bit to be one or zero are equal.

The algorithm we are analyzing is in fact deterministic, but it depends on the random selection of a hash function. This hash function is the component which let's us consider our input stream as a stream of pair-wise independent, uniformly distributed random numbers in a known range. And the algorithm depends heavily on this property.

The proof that the pairwise-independence of the transformed input string is easy and should be presented. Maybe ten minutes is enough.
 
It could be good to give more examples of the application of $k$-pairwise independence.




\subsection{Exercises}


\begin{enumerate}

\item Give an algorithm that solves the distinct elements problem in $O(|S|)$ time
and $O(n)$ memory. Prove its correctness.

\item Run Alon et al.' algorithm with $S = 1 2 3 3 5 5 1 2 5$ and $a=2$ and $b=3$.
Show all calculations in every while loop iteration.

\item In you favorite programing language, write Alon et al.'s algorithm. 
Run 10 experiments for $n=10$ and $N=100$ with random streams.
How far is the algorithm from the solution?  
\end{enumerate}



\section{The surprise index of a stream}



\begin{thebibliography}{XXX}

\bibitem{Ref1} . N. Alon, Y. Matias, M. Azegedy. The Space Complexity of Approximating the Frequency Moments. J. Computer Systems and Sciences. 58:(137--147), 1999.

Let's add books. Some recommendations?

\end{thebibliography}

\end{document}  