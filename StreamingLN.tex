\documentclass[12pt]{article}


\usepackage{parskip} \setlength{\parindent}{15pt}


\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
%\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{claim}{Claim}
\newtheorem{remark}{Remark}
\newtheorem{question}{Question}
\newtheorem{definition}{Definition}


\title{Streaming Algorithms}
\author{The Authors}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle


%%---------------------------------------------------------------------
\section{Rewarding Subway's users}

\subsection{The problem}

\subsection{What is Big Data?}

\subsection{The streaming model}

\subsection{A simple algorithm}

\subsection{Exercises}


%%---------------------------------------------------------------------
\section{Heavy hitters {\small [Manku and Motwani 2002]}}


\subsection{The problem}

\subsection{A warm up: find the majority element}


\subsection{A heavy hitters algorithm}


\subsection{Exercises}


%%---------------------------------------------------------------------

\section{Length of the stream with few space {\small [Morris 1978, Flajolet 1985]}}

\subsection{Definition of the problem}

\subsection{Applications}

\subsection{A simple implementation (not in the streaming model)}

\subsection{Morris's algorithm} $\,$

\subsection{Some ideas to take away from Flajolet's analysis}

\subsection{Exercises}


%%---------------------------------------------------------------------
\section{Number of distinct elements}

\subsection{The problem}

\begin{definition}[The number of distinct elements problem]
The input is an stream $S$ with $m$ elements, each taken from $\{1, \hdots, n\}$.
After reading the stream once, we want to know the number of distinct elements in $S$.
\end{definition}

This problem is a particular case of the so-called \emph{frequency moments of} $S$:


\begin{definition}[Frequency moments of a stream]
Let $S = s_1, \hdots, s_m$ be a string with $m$ elements, each taken from $\{1, \hdots, n\}$.
Let $m_i$ be the number of apparences of $i$ in $S$, namely, $m_i = | \{j : s_j = i\} |$.
For every integer $k \geq 0$, the \emph{$k$-th frequency moment of $S$} is

$$F_k = \Sigma^n_{i = n} m_i^k.$$ 
\end{definition}






Of course, the problem is easy if there is no restriction on the total memory.
Insist that the total memory must be polylog in $n$.

Introduce $F_k$ and say that the problem is to compute $F_0$.



\subsection{A simple solution}
If there is no restriction on the memory an algorithm can use, the problem can be easily solved:






\subsection{An algorithm}

Alon et al.'s solution.

\subsection{What is the algorithm computing?}

The order of magnitude interpretation (with a table).


\subsection{Not a formal analysis but so ideas to take away}

Intuitively, the algorithm works because:
\begin{itemize}
\item On one hand, the probability that \emph{one} hashed number has $r$ zero right-most  bits, decreases exponentiallr fast on $r$.
\item on the other hand, the probability that \emph{some} hashed number has $r$ zero right-most bits, increases exponentially fast on $r$.
\end{itemize}
These two probabilities are \emph{balanced} in such a way that that the probability of giving the right answer is "high".

The exact details are of algebraic nature and can be presented within 30 minutes, taking a slow pace to make sure the students can understand. Of course we are assuming they have almost no previous exposure to probability, just as yours truly.

Markov's and Chebyshev's inequalitys are used to bound the probability that the algorithm fails.

\subsubsection{pairwise independence}

Something not used as often as Markov's and Chebyshev's inequalitys, which is used and is very important 
is the concept of \emph{pairwise independence}. During the proof, the variance of one random variable is calculated. 
This variance depends on the fact that the probabilities that two distinct input numbers $a_i$ and $a_j$ are both mapped to numbers with at least $r$ zero right-most bits, are independent.

In this particular algorithm, this is guaranteed because of the nature of our hashing functions.

To understand what a pair-wise (or $k$-wise independent)  random distribution is, is easier to first see an example of a distribution that is not. 
Consider a  random variable $X$ of two random bits, each bit having uniform probability for having one of zero.

We can define $X$ as having a value of the set $\{00,11\}$ with uniform probability. This random variable satisfies the required property. Nevertheles, the two bits of $X$ are always equal. The probability of $X$ is not uniformly distributed on the set of two bit values $\{00,01,10,11\}$.
Thus, $X$ has no more \emph{randomness} than a single bit random variable.

Now consider $X$ defined as having a value of the set $\{00,01,10,11\}$ with uniform probability. Now, the selection of the first bit can be seen as completely independent of the selection of the second bit: given the value of the first bit, the probabilities  of the second bit to be one or zero are equal.

The algorithm we are analyzing is in fact deterministic, but it depends on the random selection of a hash function. This hash function is the component which let's us consider our input stream as a stream of pair-wise independent, uniformly distributed random numbers in a known range. And the algorithm depends heavily on this property.

The proof that the pairwise-independence of the transformed input string is easy and should be presented. Maybe ten minutes is enough.
 
It could be good to give more examples of the application of $k$-pairwise independence.


\subsection{Excersises}



\begin{thebibliography}{XXX}

\bibitem{Ref1} . N. Alon, Y. Matias, M. Azegedy. The Space Complexity of Approximating the Frequency Moments. J. Computer Systems and Sciences. 58:(137--147), 1999.

Let's add books. Some recommendations?

\end{thebibliography}

\end{document}  